{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV, train_test_split\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BalancedRandomForestClassifier\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import recall_score, f1_score, make_scorer, precision_score, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"AlarmGrundlag_ModelParametre_Merged1.1.csv\", delimiter=\";\")\n",
    "data = data.drop(columns=[\"Customer_Refnr\", \"RUN_DATE\", \"CASE_CLOSE_DATE\", \"SCENARIO_NAME\", \"ALERT_ID\", \"CASE_ID\", \"Customer_Risk_Profile_Current\"])\n",
    "data['CASE_STATUS_CODE'] = data['CASE_STATUS_CODE'].replace({'C': 0, 'R': 1})\n",
    "data.dropna(subset=['Customer_Risk_Profile_BeforeAlert'], inplace=True)\n",
    "data = pd.get_dummies(data, columns=['Customer_Risk_Profile_BeforeAlert'], prefix='RiskGroup')\n",
    "# Replace infinities with NaN for easier handling\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with any NaNs that might have been infinities initially\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11600 entries, 0 to 11601\n",
      "Data columns (total 85 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   Customer_Age                               11600 non-null  int64  \n",
      " 1   CASE_STATUS_CODE                           11600 non-null  int64  \n",
      " 2   ALERT_Triggered_In_HolidayPeriod           11600 non-null  int64  \n",
      " 3   Customer_can_Overdraft                     11600 non-null  int64  \n",
      " 4   Customer_has_Loan                          11600 non-null  int64  \n",
      " 5   Customer_has_Depot                         11600 non-null  int64  \n",
      " 6   Customer_has_InstallmentPension            11600 non-null  int64  \n",
      " 7   Customer_has_SelfPension                   11600 non-null  int64  \n",
      " 8   Customer_has_BusinessAccount               11600 non-null  int64  \n",
      " 9   Customer_Gender                            11600 non-null  int64  \n",
      " 10  Customer_Large_Deposits                    11600 non-null  float64\n",
      " 11  Customer_Large_Withdrawals                 11600 non-null  float64\n",
      " 12  Customer_since_Number_of_Days              11600 non-null  int64  \n",
      " 13  Customer_is_new                            11600 non-null  int64  \n",
      " 14  Customer_Previously_Reported               11600 non-null  int64  \n",
      " 15  Customer_dual_citizenship                  11600 non-null  int64  \n",
      " 16  Customer_nonDK_citizenship                 11600 non-null  int64  \n",
      " 17  MobilePay_Debit_Count                      11600 non-null  int64  \n",
      " 18  MobilePay_Debit_SumDKK                     11600 non-null  float64\n",
      " 19  MobilePay_Debit_AvgDKK                     11600 non-null  float64\n",
      " 20  MobilePay_Credit_Count                     11600 non-null  int64  \n",
      " 21  MobilePay_Credit_SumDKK                    11600 non-null  float64\n",
      " 22  MobilePay_Credit_AvgDKK                    11600 non-null  float64\n",
      " 23  MobilePay_Count_DebitCreditRatio           11600 non-null  float64\n",
      " 24  MobilePay_Sum_DebitCreditRatio             11600 non-null  float64\n",
      " 25  Cash_HighRiskCountry_Debit_Count           11600 non-null  int64  \n",
      " 26  Cash_HighRiskCountry_Debit_SumDKK          11600 non-null  float64\n",
      " 27  Cash_MediumRiskCountry_Debit_Count         11600 non-null  int64  \n",
      " 28  Cash_MediumRiskCountry_Debit_SumDKK        11600 non-null  float64\n",
      " 29  Cash_LowRiskCountry_Debit_Count            11600 non-null  int64  \n",
      " 30  Cash_LowRiskCountry_Debit_SumDKK           11600 non-null  float64\n",
      " 31  Cash_DNK_Debit_Count                       11600 non-null  int64  \n",
      " 32  Cash_DNK_Debit_SumDKK                      11600 non-null  float64\n",
      " 33  Cash_Debit_TotalCount                      11600 non-null  int64  \n",
      " 34  Cash_Debit_TotalSumDKK                     11600 non-null  float64\n",
      " 35  Cash_Deposits_SumDKK                       11600 non-null  float64\n",
      " 36  Cash_Deposits_Count                        11600 non-null  int64  \n",
      " 37  Card_Store_HighRiskCountry_Debit_SumDKK    11600 non-null  float64\n",
      " 38  Card_Store_HighRiskCountry_Debit_Count     11600 non-null  int64  \n",
      " 39  Card_Store_MediumRiskCountry_Debit_SumDKK  11600 non-null  float64\n",
      " 40  Card_Store_MediumRiskCountry_Debit_Count   11600 non-null  int64  \n",
      " 41  Card_Store_LowRiskCountry_Debit_SumDKK     11600 non-null  float64\n",
      " 42  Card_Store_LowRiskCountry_Debit_Count      11600 non-null  int64  \n",
      " 43  Card_Store_DNK_Debit_SumDKK                11600 non-null  float64\n",
      " 44  Card_Store_DNK_Debit_Count                 11600 non-null  int64  \n",
      " 45  Card_Online_Debit_SumDKK                   11600 non-null  float64\n",
      " 46  Card_Online_Debit_Count                    11600 non-null  int64  \n",
      " 47  Card_Online_Credit_SumDKK                  11600 non-null  float64\n",
      " 48  Card_Online_Credit_Count                   11600 non-null  int64  \n",
      " 49  ForeignCurrency_Bought_SumDKK              11600 non-null  float64\n",
      " 50  ForeignCurrency_Bought_Count               11600 non-null  int64  \n",
      " 51  Exceeded_Cash_Yearly_Debit                 11600 non-null  int64  \n",
      " 52  Exceeded_Cash_Yearly_Credit                11600 non-null  int64  \n",
      " 53  Exceeded_Foreign_Yearly_Debit              11600 non-null  int64  \n",
      " 54  Exceeded_Foreign_Yearly_Credit             11600 non-null  int64  \n",
      " 55  Exceeded_3mo_Debit                         11600 non-null  int64  \n",
      " 56  Exceeded_3mo_Credit                        11600 non-null  int64  \n",
      " 57  Exceeded_Foreign_3mo_Debit                 11600 non-null  int64  \n",
      " 58  Exceeded_Foreign_3mo_Credit                11600 non-null  int64  \n",
      " 59  Loan_3mo_Debit_Count                       11600 non-null  int64  \n",
      " 60  Loan_3mo_Debit_SumDKK                      11600 non-null  float64\n",
      " 61  Loan_3mo_Credit_Count                      11600 non-null  int64  \n",
      " 62  Loan_3mo_Credit_SumDKK                     11600 non-null  float64\n",
      " 63  Debit_12mo_SumDKK                          11600 non-null  float64\n",
      " 64  Debit_12mo_Count                           11600 non-null  int64  \n",
      " 65  Credit_12mo_SumDKK                         11600 non-null  float64\n",
      " 66  Credit_12mo_Count                          11600 non-null  int64  \n",
      " 67  Debit_3mo_SumDKK                           11600 non-null  float64\n",
      " 68  Debit_3mo_Count                            11600 non-null  int64  \n",
      " 69  Credit_3mo_SumDKK                          11600 non-null  float64\n",
      " 70  Credit_3mo_Count                           11600 non-null  int64  \n",
      " 71  Express_12mo_Debit_SumDKK                  11600 non-null  float64\n",
      " 72  Express_12mo_Debit_Count                   11600 non-null  int64  \n",
      " 73  Express_Ratio_SumDKK                       11600 non-null  float64\n",
      " 74  Express_Ratio_Count                        11600 non-null  float64\n",
      " 75  Gambling_Debit_SumDKK                      11600 non-null  float64\n",
      " 76  Gambling_Debit_Count                       11600 non-null  int64  \n",
      " 77  Gambling_Debit_AvgDKK                      11600 non-null  float64\n",
      " 78  Gambling_Credit_SumDKK                     11600 non-null  float64\n",
      " 79  Gambling_Credit_Count                      11600 non-null  int64  \n",
      " 80  Gambling_Credit_AvgDKK                     11600 non-null  float64\n",
      " 81  RiskGroup_1.0                              11600 non-null  bool   \n",
      " 82  RiskGroup_2.0                              11600 non-null  bool   \n",
      " 83  RiskGroup_3.0                              11600 non-null  bool   \n",
      " 84  RiskGroup_4.0                              11600 non-null  bool   \n",
      "dtypes: bool(4), float64(34), int64(47)\n",
      "memory usage: 7.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Express_Ratio_SumDKK</th>\n",
       "      <th>Express_Ratio_Count</th>\n",
       "      <th>MobilePay_Count_DebitCreditRatio</th>\n",
       "      <th>MobilePay_Sum_DebitCreditRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11600.000000</td>\n",
       "      <td>11600.000000</td>\n",
       "      <td>11600.000000</td>\n",
       "      <td>11600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.043850</td>\n",
       "      <td>1.154982</td>\n",
       "      <td>0.889737</td>\n",
       "      <td>1.810119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107714</td>\n",
       "      <td>4.129990</td>\n",
       "      <td>7.953707</td>\n",
       "      <td>49.077408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.152138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.024511</td>\n",
       "      <td>0.366664</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.867546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999187</td>\n",
       "      <td>82.022472</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>5104.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Express_Ratio_SumDKK  Express_Ratio_Count  \\\n",
       "count          11600.000000         11600.000000   \n",
       "mean               0.043850             1.154982   \n",
       "std                0.107714             4.129990   \n",
       "min                0.000000             0.000000   \n",
       "25%                0.000000             0.000000   \n",
       "50%                0.000000             0.000000   \n",
       "75%                0.024511             0.366664   \n",
       "max                0.999187            82.022472   \n",
       "\n",
       "       MobilePay_Count_DebitCreditRatio  MobilePay_Sum_DebitCreditRatio  \n",
       "count                      11600.000000                    11600.000000  \n",
       "mean                           0.889737                        1.810119  \n",
       "std                            7.953707                       49.077408  \n",
       "min                            0.000000                        0.000000  \n",
       "25%                            0.000000                        0.000000  \n",
       "50%                            0.155172                        0.152138  \n",
       "75%                            0.662100                        0.867546  \n",
       "max                          434.000000                     5104.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Express_Ratio_SumDKK', 'Express_Ratio_Count', 'MobilePay_Count_DebitCreditRatio', 'MobilePay_Sum_DebitCreditRatio']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['CASE_STATUS_CODE']\n",
    "X = data.drop('CASE_STATUS_CODE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_log = ['Express_Ratio_SumDKK', 'Express_Ratio_Count', 'MobilePay_Count_DebitCreditRatio', 'MobilePay_Sum_DebitCreditRatio']\n",
    "for column in columns_to_log:\n",
    "    data[column] = np.log1p(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)  # Use the same scaler instance\n",
    "# X_test_scaled = scaler.transform(X_test)  # Use the same scaler instance\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_val_pca = pca.transform(X_val_scaled)  # Transform validation data using the same PCA model\n",
    "# X_test_pca = pca.transform(X_test_scaled)  # Transform test data using the same PCA model\n",
    "\n",
    "# print(f'Percentage - X_train_pca: {len(X_train)/len(X)}, X_val_pca: {len(X_val)/len(X)}, X_test_pca: {len(X_test)/len(X)}')\n",
    "# print(f\"Shapes - X_train_pca: {X_train.shape}, X_val_pca: {X_val.shape}, X_test_pca: {X_test.shape}\")\n",
    "# print(f\"Shapes - y_train: {y_train.shape}, y_val: {y_val.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework using predefined validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating XGBoost:  17%|█▋        | 17/100 [00:06<00:32,  2.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Fit the model on training data\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Predict and evaluate on validation data\u001b[39;00m\n\u001b[0;32m     83\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\sklearn.py:1519\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1491\u001b[0m (\n\u001b[0;32m   1492\u001b[0m     model,\n\u001b[0;32m   1493\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1499\u001b[0m )\n\u001b[0;32m   1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1501\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1502\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1517\u001b[0m )\n\u001b[1;32m-> 1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:2051\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     _check_call(\n\u001b[1;32m-> 2051\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2054\u001b[0m     )\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Define columns to log-transform\n",
    "columns_to_log = ['Express_Ratio_SumDKK', 'Express_Ratio_Count', 'MobilePay_Count_DebitCreditRatio', 'MobilePay_Sum_DebitCreditRatio']\n",
    "\n",
    "# Function to apply log transformation\n",
    "def log_transform(x):\n",
    "    return np.log1p(x)\n",
    "\n",
    "# Create a transformer that applies log transformation\n",
    "log_transformer = FunctionTransformer(np.log1p)\n",
    "\n",
    "# Define the column transformer & PCA\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', make_pipeline(FunctionTransformer(np.log1p), PCA(n_components=0.95, random_state=42)), columns_to_log)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Assuming X_train, X_val, y_train, y_val are already defined\n",
    "# Define the parameter grids\n",
    "param_grid_lr = {\n",
    "    'preprocessor__num__pca__n_components': [0.75, 0.85, 0.95],  # Explained variance ratios\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "param_grid_brf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': [{0: 1, 1: v} for v in [1, 2, 3, 4, 5, 10, 20]]\n",
    "}\n",
    "param_grid_rf_smote = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20, 30],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'randomforestclassifier__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "param_grid_rf_adasyn = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20, 30],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'randomforestclassifier__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [1, 5, 10], \n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0], \n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3] \n",
    "}\n",
    "\n",
    "# Define model configurations using pipelines that include log transformation and PCA\n",
    "models = [\n",
    "    #(BalancedRandomForestClassifier(random_state=42), param_grid_brf, 'Balanced Random Forest'),\n",
    "    #(make_imb_pipeline(SMOTE(random_state=42), RandomForestClassifier(random_state=42, class_weight='balanced')), param_grid_rf_smote, 'Random Forest with SMOTE'),\n",
    "    #(make_imb_pipeline(ADASYN(random_state=42), RandomForestClassifier(random_state=42, class_weight='balanced')), param_grid_rf_adasyn, 'Random Forest with ADASYN'),\n",
    "    #(Pipeline([('preprocessor', preprocessor), ('classifier', LogisticRegression(random_state=42, max_iter=1000))]), param_grid_lr, 'Logistic Regression'),\n",
    "    (XGBClassifier(random_state=42, objective='binary:logistic'), param_grid_xgb, 'XGBoost')\n",
    "]\n",
    "\n",
    "# Initialize best results for each model\n",
    "best_results = {name: {'score': 0, 'params': {}, 'model': None, 'recall': 0, 'precision': 0, 'threshold': 0, 'predictions': None} for _, _, name in models}\n",
    "\n",
    "# Loop through each model and parameter grid\n",
    "for model, param_grid, model_name in models:\n",
    "    # Create all possible combinations of the current model's parameter grid\n",
    "    all_combinations = list(itertools.product(*param_grid.values()))\n",
    "    \n",
    "    # Randomly sample 100 combinations from the list of all combinations, if there are enough\n",
    "    selected_combinations = random.sample(all_combinations, min(100, len(all_combinations)))\n",
    "    \n",
    "    # Evaluate each combination\n",
    "    for combo in tqdm(selected_combinations, desc=f\"Evaluating {model_name}\"):\n",
    "        params = dict(zip(param_grid.keys(), combo))\n",
    "        model.set_params(**params)\n",
    "        \n",
    "        # Fit the model on training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate on validation data\n",
    "        probabilities = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Evaluate different thresholds\n",
    "        for threshold in np.linspace(0.1, 0.9, 41):\n",
    "            predictions = (probabilities >= threshold).astype(int)\n",
    "            recall = recall_score(y_val, predictions, pos_label=1)\n",
    "            auc_prc = average_precision_score(y_val, probabilities)\n",
    "            \n",
    "            # Update best results if current model's AUC-PRC is better and recall is above 90%\n",
    "            if auc_prc > best_results[model_name]['score']: # and recall >= 0.90\n",
    "                best_results[model_name]['score'] = auc_prc\n",
    "                best_results[model_name]['recall'] = recall\n",
    "                best_results[model_name]['precision'] = precision_score(y_val, predictions, pos_label=1)\n",
    "                best_results[model_name]['threshold'] = threshold\n",
    "                best_results[model_name]['params'] = params\n",
    "                best_results[model_name]['model'] = model\n",
    "                best_results[model_name]['predictions'] = predictions\n",
    "\n",
    "# Output the best results for each model configuration\n",
    "for model_name, details in best_results.items():\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Best auc_prc score: {details['score']}\")\n",
    "    print(f\"Best recall achieved: {details['recall']}\")\n",
    "    print(f\"Best precision achieved: {details['precision']}\")\n",
    "    print(f\"Optimal threshold for predictions: {details['threshold']}\")\n",
    "    print(f\"Best parameters found: {details['params']}\")\n",
    "    print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating XGBoost on fold: 100%|██████████| 100/100 [08:51<00:00,  5.31s/it]\n",
      "Evaluating XGBoost on fold: 100%|██████████| 100/100 [09:04<00:00,  5.45s/it]\n",
      "Evaluating XGBoost on fold: 100%|██████████| 100/100 [09:34<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Best results for XGBoost:\n",
      "Best auc_prc score: 0.8351337546601176\n",
      "Best recall achieved: 0.7714285714285715\n",
      "Best precision achieved: 0.6878980891719745\n",
      "Optimal threshold for predictions: 0.1\n",
      "Best parameters found: {'classifier__colsample_bytree': 0.4, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 50, 'classifier__min_child_weight': 0.001, 'classifier__n_estimators': 200, 'classifier__subsample': 1.0}\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import recall_score, precision_score, average_precision_score\n",
    "import itertools\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Define your data (assuming X_train_df and y_train_df are pandas DataFrames)\n",
    "X_train_values = X_train.values\n",
    "y_train_values = y_train.values.ravel()\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "k = 3\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'classifier__max_depth': [10, 50, 75, 100],\n",
    "    'classifier__min_child_weight': [0.001, 0.05, 0.1, 0.5, 1, 5, 10], \n",
    "    'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.2, 0.4, 0.6, 0.8, 1.0], \n",
    "    'classifier__n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Initialize best results for XGBoost\n",
    "best_results_xgb = {'score': 0, 'params': {}, 'recall': 0, 'precision': 0, 'threshold': 0, 'predictions': None}\n",
    "\n",
    "# Setup the pipeline with SMOTE and XGBoost classifier\n",
    "smote = SMOTE(random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(objective='binary:logistic', eval_metric='aucpr', seed=42)\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', smote),\n",
    "    ('classifier', xgb_clf)\n",
    "])\n",
    "\n",
    "# Loop through each fold for cross-validation\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    X_train_cv, X_val_cv = X_train_values[train_index], X_train_values[val_index]\n",
    "    y_train_cv, y_val_cv = y_train_values[train_index], y_train_values[val_index]\n",
    "    \n",
    "    # Create all possible combinations of the XGBoost parameter grid\n",
    "    all_combinations = list(itertools.product(*[param_grid_xgb[key] for key in sorted(param_grid_xgb.keys())]))\n",
    "    \n",
    "    # Randomly sample 100 combinations from the list of all combinations, if there are enough\n",
    "    selected_combinations = random.sample(all_combinations, min(100, len(all_combinations)))\n",
    "    \n",
    "    # Evaluate each combination\n",
    "    for combo in tqdm(selected_combinations, desc=\"Evaluating XGBoost on fold\"):\n",
    "        params = dict(zip(sorted(param_grid_xgb.keys()), combo))\n",
    "        pipeline.set_params(**params)\n",
    "        \n",
    "        # Fit the pipeline on training data\n",
    "        pipeline.fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        # Predict probabilities on validation data\n",
    "        probabilities = pipeline.predict_proba(X_val_cv)[:, 1]\n",
    "        \n",
    "        # Evaluate different thresholds\n",
    "        for threshold in np.linspace(0.1, 0.9, 41):\n",
    "            predictions = (probabilities >= threshold).astype(int)\n",
    "            recall = recall_score(y_val_cv, predictions)\n",
    "            auc_prc = average_precision_score(y_val_cv, probabilities)\n",
    "            \n",
    "            # Update best results if current model's AUC-PRC is better and recall is above 90%\n",
    "            if auc_prc > best_results_xgb['score']:\n",
    "                best_results_xgb['score'] = auc_prc\n",
    "                best_results_xgb['recall'] = recall\n",
    "                best_results_xgb['precision'] = precision_score(y_val_cv, predictions)\n",
    "                best_results_xgb['threshold'] = threshold\n",
    "                best_results_xgb['params'] = params\n",
    "                best_results_xgb['predictions'] = predictions\n",
    "                best_results_xgb['model'] = pipeline\n",
    "\n",
    "# Output the best results for XGBoost\n",
    "print('----------------------------------------------------------------------')\n",
    "print(\"Best results for XGBoost:\")\n",
    "print(f\"Best auc_prc score: {best_results_xgb['score']}\")\n",
    "print(f\"Best recall achieved: {best_results_xgb['recall']}\")\n",
    "print(f\"Best precision achieved: {best_results_xgb['precision']}\")\n",
    "print(f\"Optimal threshold for predictions: {best_results_xgb['threshold']}\")\n",
    "print(f\"Best parameters found: {best_results_xgb['params']}\")\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piciu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:160: UserWarning: [01:18:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__learning_rate\", \"classifier__max_depth\", \"classifier__min_child_weight\", \"classifier__n_estimators\", \"classifier__subsample\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for the Best Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1900\n",
      "           1       0.86      0.67      0.76       420\n",
      "\n",
      "    accuracy                           0.92      2320\n",
      "   macro avg       0.90      0.83      0.85      2320\n",
      "weighted avg       0.92      0.92      0.92      2320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifier__colsample_bytree = best_results_xgb['params']['classifier__colsample_bytree']\n",
    "classifier__learning_rate = best_results_xgb['params']['classifier__learning_rate']\n",
    "classifier__max_depth = best_results_xgb['params']['classifier__max_depth']\n",
    "classifier__min_child_weight = best_results_xgb['params']['classifier__min_child_weight']\n",
    "classifier__n_estimators = best_results_xgb['params']['classifier__n_estimators']\n",
    "classifier__subsample = best_results_xgb['params']['classifier__subsample']\n",
    "\n",
    "# Train the best model on the entire training dataset\n",
    "best_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='aucpr', classifier__colsample_bytree=classifier__colsample_bytree, \n",
    "                               classifier__learning_rate=classifier__learning_rate, classifier__max_depth=classifier__max_depth, classifier__min_child_weight=classifier__min_child_weight,\n",
    "                               classifier__n_estimators=classifier__n_estimators, classifier__subsample=classifier__subsample)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report for the Best Model:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for XGBoost on Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      1882\n",
      "           1       0.82      1.00      0.90       438\n",
      "\n",
      "    accuracy                           0.96      2320\n",
      "   macro avg       0.91      0.97      0.94      2320\n",
      "weighted avg       0.97      0.96      0.96      2320\n",
      "\n",
      "\n",
      "\n",
      "0.9999320790747918\n"
     ]
    }
   ],
   "source": [
    "# Assuming best_results is populated and contains the best model, threshold, parameters, and predictions\n",
    "for model_name, details in best_results.items():\n",
    "    if details['model'] is not None and details['predictions'] is not None:\n",
    "        # Use the saved predictions from the validation set evaluation\n",
    "        validation_predictions = details['predictions']\n",
    "        \n",
    "        # Generate and print the classification report for the saved validation set predictions\n",
    "        print(f\"Classification Report for {model_name} on Validation Data:\")\n",
    "        print(classification_report(y_val, validation_predictions))\n",
    "        print(\"\\n\")  # Add a newline for better readability between reports\n",
    "        print(details['score'])\n",
    "    else:\n",
    "        print(f\"No model or predictions available for {model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
